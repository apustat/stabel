% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/stabel.vs.R
\name{stabel.vs}
\alias{stabel.vs}
\title{Variable selection using stabel}
\usage{
stabel.vs(
  X,
  Y,
  cutoff = 0.6,
  B = 50,
  bestlam.Lasso = NULL,
  maxit.Lasso = 1e+05,
  family = "binomial",
  maxit.sparseSVM = 1000,
  dfmax = p + 1,
  gamma = NULL,
  bestlam.sparseSVM = NULL,
  ntree = 200,
  mcAdj = TRUE,
  maxRuns = 100,
  pValue = 0.01,
  fit.fun = c("Lasso", "sparseSVM"),
  comb.method = "average",
  seed = NULL
)
}
\arguments{
\item{X}{Input matrix.}

\item{Y}{Outcome vector.}

\item{cutoff}{Threshold of selection probability. Default is 0.6.}

\item{B}{Number of subsamples. Total 2B number of subsamples. Default is 50.}

\item{bestlam.Lasso}{Cross-validated lambda for Lasso. If fit.fun includes \code{"Lasso"}, then \code{bestlam.Lasso} must be specified,  otherwise it will throw an error.}

\item{maxit.Lasso}{Maximum number of passes over the data for all lambda values. Default is \eqn{10^5}}

\item{family}{A description of the error distribution and link function to be used in the model. Even though Lasso and RF work for both \code{"gaussian"} and \code{"binomial"}, sparseSVM works only for \code{"binomial"} family. Default is \code{"binomial"}.}

\item{maxit.sparseSVM}{Maximum number of iterations for sparseSVM. Default is 1000.}

\item{dfmax}{Limit the maximum number of variables in the model for both Lasso and sparseSVM. Default is p+1, p is the number of predictors.}

\item{gamma}{The tuning parameter for huberization smoothing of hinge loss. Default is 0.1.}

\item{bestlam.sparseSVM}{Cross-validated lambda for sparseSVM. If fit.fun includes \code{"sparseSVM"}, then \code{bestlam.sparseSVM} must be specified; otherwise it will throw an error.}

\item{ntree}{Number of trees in the forest.}

\item{mcAdj}{If set to TRUE, a multiple comparisons adjustment using the Bonferroni method will be applied. Default is \code{TRUE}.}

\item{maxRuns}{Number of importance source runs. You may increase it to resolve attributes left Tentative. Default is 100.}

\item{pValue}{Confidence level for RF. Default is 0.01.}

\item{fit.fun}{Allows users to perform variable selection using stability selection with \code{"Lasso"}, \code{"sparseSVM"}, \code{"RF"} or any combination of these methods. Default is \code{c("Lasso", "sparseSVM")}.}

\item{comb.method}{For combining two or variable selection methods. Currently allows either \code{"average"} which takes an average of the empirical selection probability or \code{"union"} which takes a union of the selected subsets. If \code{fit.fun} includes only one method, then \code{comb.method} should be specified as \code{NULL}, otherwise it will throw an error.}

\item{seed}{A random seed for reproducibility of results.}
}
\value{
A list containing the following components:
\item{selection.probabilities}{A list of multiple vectors of selection probabilities for each variable, indicating their importance across subsamples.}
\item{selected.variables}{A list of multiple vectors of selected variables based on the defined cutoff criteria.}
\item{final.selected.set}{The final set of variables selected after combining results from multiple methods.}
\item{cutoff}{Same as above.}
\item{B}{Same as above.}
\item{bestlam.Lasso}{The optimal regularization parameter (\eqn{\lambda}) for the Lasso model, determined via cross-validation.}
\item{maxit.Lasso}{Same as above.}
\item{family}{Same as above.}
\item{maxit.sparseSVM}{Same as above.}
\item{dfmax}{Same as above.}
\item{gamma}{Same as above.}
\item{bestlam.sparseSVM}{The optimal regularization parameter (\eqn{\lambda}) for the sparseSVM model, determined via cross-validation.}
\item{ntree}{Same as above.}
\item{mcAdj}{Same as above.}
\item{maxRuns}{Same as above.}
\item{pValue}{Same as above.}
\item{fit.fun}{Same as above.}
\item{comb.method}{Same as above.}
\item{seed}{Same as above.}
}
\description{
The \code{stabel.vs} function performs variable selection using stability selection in combination with three statistical methods: Lasso, sparseSVM, and Random Forest (RF). It implements the stability selection framework proposed by Shah and Samworth (2013), considering no assumption, to identify robust subsets of variables. The function then combines the selected subsets from these methods through ensemble learning. One can use any individual models or any pair or all three.
}
\examples{
stabel.vs()
}
\keyword{stabel}
